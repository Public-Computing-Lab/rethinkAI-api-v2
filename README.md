# RethinkAI - Community Sentiment Analysis Platform

A comprehensive AI-powered platform for analyzing and understanding community sentiment around public safety in the Dorchester community of Boston. This project combines traditional data science approaches with LLM-based chat interactions to explore and make sense of community data.

## ğŸ¯ Project Goals

This platform enables:
- **Interactive Data Exploration**: Query 311 requests, crime reports, and community events through natural language
- **Community Engagement**: Access community newsletters, meeting transcripts, and policy documents via semantic search
- **Intelligent Question Routing**: Automatically routes questions to SQL queries (structured data) or RAG retrieval (documents) or both (hybrid mode)
- **Event Discovery**: Find upcoming community events with temporal queries

## ğŸ“ Project Structure

```
rethinkAI-api-v2/
â”œâ”€â”€ api/                          # Flask REST API (v2.0)
â”‚   â”œâ”€â”€ api.py                    # Main API endpoint (agent-powered)
â”‚   â”œâ”€â”€ datastore/                # Static data files
â”‚   â””â”€â”€ prompts/                  # LLM prompt templates
â”‚
â”œâ”€â”€ credentials/                  # Google credential store    
â”‚
â”œâ”€â”€ main_chat/                    # Core chatbot and data processing
â”‚   â”œâ”€â”€ chat_route.py             # Main chatbot orchestration
â”‚   â”œâ”€â”€ sql_pipeline/             # SQL query generation and execution
â”‚   â”œâ”€â”€ rag_pipeline/             # RAG retrieval system
â”‚   â”œâ”€â”€ data_ingestion/           # Automated data sync (Google Drive, Email)
â”‚   â”œâ”€â”€ calendar/                 # Event extraction and processing
â”‚   â””â”€â”€ new_metadata/             # Database schema metadata generation
â”‚
â”œâ”€â”€ dataset-documentation/        # Dataset documentation (see below)
â”œâ”€â”€ public/                       # Frontend webapp
â”œâ”€â”€ scripts/                      # Deployment and Maintenance Scripts
â””â”€â”€ requirements.txt              # API dependencies
```

## ğŸš€ Quick Start

### Demo-Friendly Setup (Dockerized MySQL, recommended for quick evals)

For instructors and evaluators, a lightweight demo setup is available in the `demo/` folder. This avoids any client credentials and uses a small demo database snapshot and vector store **without running the data ingestion pipeline**, since ingestion requires additional setup of Google Drive and Gmail credentials.

To keep setup instructions in one place (and avoid the main README getting out of sync with the actual scripts), **all demo-specific setup steps are documented in** `demo/README.md`.  

From the project root, see:

- `demo/README.md` â€“ how to:
  - Run `demo/setup.sh` or `demo/setup_windows.bat`
  - Bring up the Dockerized MySQL demo database
  - Configure the minimal `.env` values needed for the demo

Once youâ€™ve followed the steps in `demo/README.md`, you can **skip the Installation section below** and just use the Configuration and Running API/frontend sections as reference.

### Prerequisites

- Python 3.11+
- MySQL 8.0+ (for structured data)
- Google Gemini API key

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd ml-misi-community-sentiment
   ```

2. **Create and activate virtual environment**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Mac/Linux
   venv\Scripts\activate     # On Windows
   ```

3. **Install dependencies**
   ```bash
   # Install all dependencies from root requirements.txt
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   - Copy `.env.example` files to `.env` in each directory
   - See [Configuration](#configuration) section below

5. **Set up database**
   - Create MySQL database: `rethink_ai_boston`
   - Run database setup scripts (see `main_chat/data_ingestion/`)

6. **Run the API**
   ```bash
   cd api
   python api.py
   ```
   The API will start on `http://127.0.0.1:8888`

7. **Run the Frontend** (in a separate terminal)
   ```bash
   # From project root
   cd public
   python -m http.server 8000
   ```
   Then open `http://localhost:8000` in your browser

  **Note**: Make sure the backend API is running before starting the frontend. The frontend connects to the API at `http://127.0.0.1:8888` by default.

## âš™ï¸ Configuration

### Environment Variables

The project uses a **single `.env` file at the repo root**.

- Copy `example_env.txt` to `.env`:
  ```bash
  cp example_env.txt .env
  ```
- Edit `.env` and fill in the values for your environment.

**Key Variables (non-exhaustive):**
- `GEMINI_API_KEY` â€“ Google Gemini API key (required)
- `RETHINKAI_API_KEYS` â€“ API authentication keys (comma-separated)
- `MYSQL_HOST`, `MYSQL_PORT`, `MYSQL_USER`, `MYSQL_PASSWORD`, `MYSQL_DB` â€“ MySQL connection
- `VECTORDB_DIR` â€“ path to the ChromaDB/vector DB directory
- `GOOGLE_DRIVE_FOLDER_ID` and related `GOOGLE_*/GMAIL_*` settings â€“ data ingestion

## ğŸ“Š Data Sources

### Structured Data (MySQL)
- **311 Requests**: Service requests from Boston 311 system
- **911 Reports**: Crime and emergency reports
- **Community Events**: Calendar events extracted from newsletters

### Unstructured Data (Vector Database)

- **Meeting Transcripts**: Community meeting notes and discussions
- **Policy Documents**: City planning documents, budgets, and initiatives

### Data Ingestion
The system automatically syncs data from:
- **Google Drive**: Client-uploaded documents (PDF, DOCX, TXT, MD)
- **Email Newsletters**: Automated extraction of events to calendar

See `main_chat/data_ingestion/README.md` for details.

## ğŸ”Œ API Endpoints

### Agent API v2.0 (`api/api.py`)

- **POST /chat** - Main chat interaction with intelligent routing
- **POST /log** - Log interactions
- **PUT /log** - Update interaction feedback
- **GET /events** - Fetch upcoming community events
- **GET /health** - Health check

See `api/README.md` for detailed API documentation.

## ğŸ—‚ï¸ Dataset Documentation

Comprehensive dataset documentation is available in the `dataset-documentation/` folder. This includes:
- Data source descriptions
- Schema documentation
- Data quality notes
- Usage examples

See `dataset-documentation/README.md` for details.

## ğŸ“ For Next Student Team

### What We've Built

This project implements a **hybrid AI system** that combines:
1. **SQL-based queries** for structured data (311, 911, events)
2. **RAG (Retrieval-Augmented Generation)** for document-based answers
3. **Intelligent routing** that selects the best approach for each question

### Key Components

1. **Unified Chatbot** (`main_chat/chat_route.py`)
   - Routes questions to SQL, RAG, or hybrid mode
   - Manages conversation history and context
   - Handles source citations

2. **Data Ingestion Pipeline** (`main_chat/data_ingestion/`)
   - Automated sync from Google Drive and email
   - Event extraction from newsletters
   - Vector database updates

3. **API Layer** (`api/api.py`)
   - RESTful endpoints for frontend integration
   - Session management
   - Interaction logging

### Recommended Next Steps

1. **Start Here**: Review `main_chat/chat_route.py` to understand the core routing logic
2. **Test the API**: Use `api/test_api.py` to test endpoints
3. **Explore Data**: Check `dataset-documentation/` for available data sources
4. **Frontend Integration**: 
   - See `public/` for the production frontend (see `public/README.md` for details)
   - Both can be used to test the API via a web interface

### Development Workflow

1. **Local Development**
   ```bash
   # Start API server
   cd api
   python api.py
   
   # Test with curl or Postman
   curl -X POST http://localhost:8888/chat \
     -H "RethinkAI-API-Key: your-key" \
     -H "Content-Type: application/json" \
     -d '{"message": "What events are happening this weekend?"}'
   ```

2. **Data Updates**
   ```bash
   # Run data ingestion
   cd main_chat/data_ingestion
   python boston_data_sync/boston_data_sync.py
   ```

3. **Database Setup**
   - See `main_chat/data_ingestion/README.md` for database initialization

### Common Issues & Solutions

- **API Key Errors**: Ensure `GEMINI_API_KEY` is set in `.env`
- **Database Connection**: Verify MySQL credentials and database exists
- **Vector DB Issues**: Check `VECTORDB_DIR` path and permissions
- **Import Errors**: Ensure virtual environment is activated and dependencies installed

### Documentation References

- API Documentation: `api/README.md`
- Data Ingestion: `main_chat/data_ingestion/README.md`
- Dataset Info: `dataset-documentation/README.md`
- API v2 Details: `main_chat/api_readme.md`

## ğŸŒ Deployment

### DreamHost Setup (needs to be tested)

See `scripts/dreamhost/` for skeleton deployment scripts:
- `setup.sh` - Initial server setup
- `deploy.sh` - Application deployment
- `database_setup.sh` - Database initialization

### Production Considerations

- Use `gunicorn` or similar WSGI server for production
- Set `FLASK_SESSION_COOKIE_SECURE=True` for HTTPS
- Configure proper CORS origins
- Set up database backups
- Monitor API usage and costs

## ğŸ“ License

See `LICENSE.md` for license information.

## ğŸ‘¥ Contact

- **Project Owner**: buspark@bu.edu
- **Repository**: [GitHub Link]

## ğŸ”— Links

- **Interactive Dashboard**: [Add dashboard URL if hosted]
- **API Documentation**: See `api/README.md`
- **Dataset Documentation**: See `dataset-documentation/README.md`

---

